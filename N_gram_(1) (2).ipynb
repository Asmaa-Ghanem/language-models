{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Process the IMDB Dataset"
      ],
      "metadata": {
        "id": "cD9ZRNeIJ_Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown\n",
        "\n",
        "# 1. Download the zipped IMDB dataset from Drive\n",
        "# this is the unsup part of https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "!gdown \"https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\" -O imdb_dataset.zip\n",
        "\n",
        "# 2. Unzip the downloaded file\n",
        "!unzip -q imdb_dataset.zip -d imdb_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0olLNujvMbg8",
        "outputId": "49d87c12-87b2-4bd7-dcd0-a214ec06f5a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\n",
            "From (redirected): https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB&confirm=t&uuid=84338637-d7ae-41bd-a969-f21bb23a139d\n",
            "To: /content/imdb_dataset.zip\n",
            "100% 44.7M/44.7M [00:00<00:00, 174MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "from math import log, exp\n"
      ],
      "metadata": {
        "id": "siDkX864cazJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G1K-8ybJJ3Cl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_imdb_unsup_sentences(folder_path):\n",
        "    \"\"\"\n",
        "    Loads text files from the IMDB 'unsup' (unsupervised) folder.\n",
        "    split text by newline, strips text, and returns a list of raw lines.\n",
        "    replace <br /> tags with special token <nl> token.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "    # Loop through all files in the directory\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)  # Get full path to file\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read().replace(\"<br />\", \" <nl> \")  # Replace <br /> with <nl>\n",
        "            lines = text.split(\"\\n\")  # Split text by newline\n",
        "            all_sentences.extend([line.strip() for line in lines if line.strip()])  # Remove empty lines\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"\n",
        "    Removes punctuation from the text,\n",
        "    but keeps <nl> tokens intact.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"[^\\w\\s<nl>]\", \"\", text)  # Remove all punctuation except <nl>\n",
        "    return text\n",
        "\n",
        "def build_vocabulary(sentences):\n",
        "    \"\"\"\n",
        "    lower each sentence,\n",
        "    Splits each sentence on whitespace, removes punctuation,\n",
        "    and builds a set of unique tokens (vocabulary).\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()  # Convert to lowercase\n",
        "        sentence = remove_punctuation(sentence)  # Remove punctuation\n",
        "        tokens = sentence.split()  # Tokenize (split words)\n",
        "        vocab.update(tokens)  # Add tokens to vocabulary\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def tokinize(sentences, vocab, unknown=\"<UNK>\"):\n",
        "    \"\"\"\n",
        "    lower each sentence,\n",
        "    Splits each sentence on whitespace, removes punctuation,\n",
        "    and replaces tokens not in the vocabulary with unknowen token.\n",
        "    Returns the list of tokenized sentences.\n",
        "    \"\"\"\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()  # Convert to lowercase\n",
        "        sentence = remove_punctuation(sentence)  # Remove punctuation\n",
        "        tokens = sentence.split()  # Tokenize (split words by spaces)\n",
        "\n",
        "        # Replace unknown words with <UNK>\n",
        "        tokens = [token if token in vocab else unknown for token in tokens]\n",
        "\n",
        "        tokenized_sentences.append(tokens)  # Add to the list\n",
        "\n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_folder = \"imdb_data/unsup\"\n",
        "sentences = load_imdb_unsup_sentences(imdb_folder)\n",
        "\n",
        "print(f\"Number of raw sentences loaded: {len(sentences)}\")\n",
        "print(f\"Example (first 2 sentences):\\n{sentences[:2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-5469mMKcRP",
        "outputId": "282f686e-7aea-4f60-8045-fae03c942c5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of raw sentences loaded: 50000\n",
            "Example (first 2 sentences):\n",
            "['At the beginning of this film, there\\'s a tight shot on Brooke Shields\\' baby face: she\\'s watching something with interest and we hear a woman moaning just in front of her. Since we all know what \"Pretty Baby\" is about, one is to assume the child is watching some sexual act with curiosity. Actually, it\\'s just the opposite. This is writer-director Louis Malle\\'s clever way of laughing at the viewer, saying \"You have the dirty mind, not I.\" It\\'s a very smart way to begin to the picture, but little else occupied my mind after it got going. Why would Keith Carradine\\'s colorless older man want to marry a pubescent prostitute? Nobody here is saying, especially not Carradine (who has one sullen expression to express every emotion). The photography and background scoring are gorgeous, however the story and characters provide no passion, no emotion. The film is like a stylish painting, but one full of dullards. *1/2 from ****', \"i quite enjoyed this film. the only problem i had with it was that i wanted to see more of gina mckee's character explored - i think all the characters were spread a little thin and it could have been better if there was more focus to fewer characters. mckee was the highlight of the film (she often is) and her storyline was especially good.  <nl>  <nl> overall i thought the family dynamics were great - in a horrifying way. the parents were terribly distressing. i found the brother storyline slightly confusing and unnecessary. i would like to have known more about him or nothing at all. <nl>  <nl> anyway, i thought it was a decent film, worth seeing by all means.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(sentences) == 50000, \"Expected 50,000 sentences from the unsup folder.\""
      ],
      "metadata": {
        "id": "Qv0J6dGhIidP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(sentences, test_split=0.1):\n",
        "    \"\"\"\n",
        "      shuffle the sentences\n",
        "      split them into train and test sets (first 1-test_split of the data is the training)\n",
        "      return the train and test sets\n",
        "    \"\"\"\n",
        "    random.shuffle(sentences)\n",
        "\n",
        "    # Compute the split index\n",
        "    split_index = int(len(sentences) * (1 - test_split))\n",
        "\n",
        "        # Split into train and test sets\n",
        "    train_sentences = sentences[:split_index]\n",
        "    test_sentences = sentences[split_index:]\n",
        "\n",
        "    return train_sentences, test_sentences\n"
      ],
      "metadata": {
        "id": "9hA3B8WEKAF_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, test_sentences = split_data(sentences)\n",
        "\n",
        "print(f\"Number of training sentences: {len(train_sentences)}\")\n",
        "print(f\"Number of test sentences: {len(test_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfYWg45aKNzP",
        "outputId": "53b3292e-01c8-4b44-af5c-99e082dbf4df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 45000\n",
            "Number of test sentences: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_sentences) == 45000, \"Expected 45,000 sentences for training.\"\n",
        "assert len(test_sentences) == 5000, \"Expected 5,000 sentences for testing.\"\n"
      ],
      "metadata": {
        "id": "i9Hh9ptkKS6Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocabulary(train_sentences)\n",
        "tokinized_sentences = tokinize(train_sentences, vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example tokens from first sentence: {tokinized_sentences[0][:10] if tokinized_sentences else 'No tokens loaded'} ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_q9qARKw_u",
        "outputId": "b11cb37a-1b1f-4f49-fbc3-6c9c8e4c9353"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 161218\n",
            "Example tokens from first sentence: ['i', 'watched', 'this', 'movie', 'just', 'for', 'the', 'sake', 'of', 'a'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assert len(vocab) == 161292, \"Expected a vocabulary size of 171,591.\" #skip for replication problems\n",
        "assert len(tokinized_sentences) == 45000, \"Expected tokenized sentences count to match raw sentences.\"\n",
        "\n",
        "example = \"I love Natural language processing, and i want to be a great engineer.\"\n",
        "assert len(example) == 70, \"Example sentence length (in characters) does not match the expected 70.\"\n",
        "\n",
        "example_tokens = tokinize([example], vocab)[0]\n",
        "assert len(example_tokens) == 13, \"Token count for the example sentence does not match the expected 13.\"\n"
      ],
      "metadata": {
        "id": "9lbynIF5K6xJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pad_sentence(tokens, n):\n",
        "    \"\"\"\n",
        "    Pads a list of tokens with <s> at the start (n-1 times)\n",
        "    and </s> at the end (once).\n",
        "    For example, if n=3, you add 2 <s> tokens at the start.\n",
        "    \"\"\"\n",
        "    padded = [\"<s>\"] * (n - 1) + tokens + [\"</s>\"]\n",
        "    return padded\n",
        "\n",
        "def build_ngram_counts(tokenized_sentences, n):\n",
        "    \"\"\"\n",
        "    Builds n-gram counts and (n-1)-gram counts from the given tokenized sentences.\n",
        "    Each sentence is padded with <s> and </s>.\n",
        "\n",
        "    Args:\n",
        "        tokenized_sentences: list of lists, where each sub-list is a tokenized sentence.\n",
        "        n: the order of the n-gram (e.g., 2 for bigrams, 3 for trigrams).\n",
        "        vocab: set of known words. If provided, you can choose to handle out-of-vocab tokens.\n",
        "\n",
        "    Returns:\n",
        "        ngram_counts: Counter of n-grams (tuples of length n).\n",
        "        context_counts: Counter of (n-1)-gram contexts.\n",
        "    \"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i:i + n])\n",
        "            context = tuple(padded_sentence[i:i + n - 1])  # (n-1)-gram context\n",
        "            #print(f\"Extracted ngram: {ngram}, Context: {context}\")\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "def laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Computes the probability of an n-gram using Laplace (add-alpha) smoothing.\n",
        "\n",
        "    P(w_i | w_{i-(n-1)}, ..., w_{i-1}) =\n",
        "        (count(ngram) + alpha) / (count(context) + alpha * vocab_size)\n",
        "\n",
        "    Args:\n",
        "        ngram: tuple of tokens representing the n-gram\n",
        "        ngram_counts: Counter of n-grams\n",
        "        context_counts: Counter of (n-1)-gram contexts\n",
        "        vocab_size: size of the vocabulary\n",
        "        alpha: smoothing parameter (1.0 = add-1 smoothing)\n",
        "\n",
        "    Returns:\n",
        "        Probability of the given n-gram.\n",
        "    \"\"\"\n",
        "    context = ngram[:-1]  # Extract (n-1)-gram context\n",
        "    ngram_count = ngram_counts.get(ngram, 0)  # Get count of the n-gram\n",
        "    context_count = context_counts.get(context, 0)  # Get count of the context\n",
        "\n",
        "    prob = (ngram_count + alpha) / (context_count + alpha * vocab_size)\n",
        "    return prob\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9bQ5K2ubNFhD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "ngram_counts, context_counts = build_ngram_counts(tokinized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFRligyRx_8",
        "outputId": "f68cd50b-c4d6-47bd-ae2c-1ea37d137f64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 6107483\n",
            "Number of contexts: 2270622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log, exp\n",
        "\n",
        "def predict_next_token(\n",
        "    context_tokens,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab,\n",
        "    n=2,\n",
        "    alpha=1.0,\n",
        "    top_k=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Given a list of context tokens, predict the next token using the n-gram model.\n",
        "    Returns the top_k predictions as (token, probability).\n",
        "    \"\"\"\n",
        "\n",
        "    candidates = []\n",
        "    if len(context_tokens) < n - 1:\n",
        "        return []  # Not enough context\n",
        "\n",
        "    context = tuple(context_tokens[-(n - 1):])  # Get the last (n-1) tokens\n",
        "\n",
        "    for word in vocab:  # Iterate over all possible next words\n",
        "        ngram = context + (word,)  # Form the n-gram\n",
        "        prob = laplace_probability(ngram, ngram_counts, context_counts, len(vocab), alpha)\n",
        "        candidates.append((word, prob))\n",
        "\n",
        "    # Sort candidates by probability in descending order and return top_k results\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return candidates[:top_k]\n",
        "\n",
        "\n",
        "def generate_text_with_limit(\n",
        "    start_tokens,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab,\n",
        "    n=2,\n",
        "    alpha=1.0,\n",
        "    max_length=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates text from an n-gram model until it sees </s>\n",
        "    or reaches a maximum total length (max_length).\n",
        "\n",
        "    Args:\n",
        "      start_tokens (list): initial context to begin generation\n",
        "      ngram_counts (Counter): trained n-gram counts\n",
        "      context_counts (Counter): trained (n-1)-gram counts\n",
        "      vocab (set): the model vocabulary\n",
        "      n (int): n-gram order, 2 for bigram, 3 for trigram, etc.\n",
        "      alpha (float): Laplace smoothing parameter\n",
        "      max_length (int): maximum number of tokens to generate (including start_tokens)\n",
        "\n",
        "    Returns:\n",
        "      A list of tokens representing the generated sequence.\n",
        "    \"\"\"\n",
        "    generated = list(start_tokens)  # Initialize with start tokens\n",
        "\n",
        "    while len(generated) < max_length:\n",
        "        context = tuple(generated[-(n - 1):])  # Get last (n-1) tokens as context\n",
        "        predictions = predict_next_token(context, ngram_counts, context_counts, vocab, n, alpha, top_k=5)\n",
        "\n",
        "        if not predictions:  # If no predictions are found, stop generation\n",
        "            break\n",
        "\n",
        "        next_token = random.choices([token for token, _ in predictions], weights=[prob for _, prob in predictions])[0]\n",
        "        #next_token = predictions[0][0]  # Select the token with the highest probability\n",
        "\n",
        "        if next_token == \"</s>\":  # Stop if end-of-sequence token is generated\n",
        "            break\n",
        "\n",
        "        generated.append(next_token)\n",
        "    return generated\n",
        "\n",
        "context = [\"i\", \"love\",\"the\",\"film\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=4,\n",
        "    alpha=1.0,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v4NzCsBMzTN",
        "outputId": "6fe6e30f-70f1-484e-cb18-e4c157ffee19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'the', 'film', 'seenitall', 'abound', 'passionate', 'booklike', 'relateable', 'seenitall', 'relateable', 'abound', 'booklike', 'booklike', 'passionate', 'abound', 'relateable', 'booklike', 'abound', 'booklike', 'seenitall', 'seenitall', 'passionate', 'relateable', 'passionate', 'passionate', 'relateable', 'relateable', 'seenitall', 'abound', 'passionate', 'booklike', 'relateable', 'passionate', 'relateable', 'passionate', 'seenitall', 'passionate', 'passionate', 'booklike', 'seenitall', 'relateable', 'seenitall', 'passionate', 'abound', 'relateable', 'booklike', 'passionate', 'booklike', 'seenitall', 'passionate', 'booklike', 'booklike', 'seenitall', 'passionate', 'abound', 'abound', 'abound', 'seenitall', 'passionate', 'passionate', 'passionate', 'abound', 'booklike', 'seenitall', 'relateable', 'relateable', 'abound', 'passionate', 'seenitall', 'booklike', 'passionate', 'abound', 'relateable', 'booklike', 'seenitall', 'seenitall', 'booklike', 'abound', 'abound', 'seenitall', 'seenitall', 'seenitall', 'abound', 'seenitall', 'abound', 'abound', 'passionate', 'abound', 'booklike', 'passionate', 'passionate', 'abound', 'seenitall', 'passionate', 'passionate', 'seenitall', 'passionate', 'relateable', 'booklike', 'passionate', 'abound', 'passionate', 'booklike', 'abound', 'seenitall', 'seenitall', 'passionate', 'relateable', 'relateable', 'abound', 'passionate', 'abound', 'abound', 'passionate', 'passionate', 'passionate', 'booklike', 'booklike', 'relateable', 'seenitall', 'seenitall', 'passionate', 'abound', 'passionate', 'abound', 'relateable', 'seenitall']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(\n",
        "    tokenized_sentences,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab_size,\n",
        "    n,\n",
        "    alpha\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of an n-gram model (with Laplace smoothing)\n",
        "    on a list of tokenized sentences.\n",
        "\n",
        "    Args:\n",
        "      tokenized_sentences: List of lists of tokens.\n",
        "      ngram_counts: Counter of n-grams.\n",
        "      context_counts: Counter of (n-1)-grams.\n",
        "      vocab_size: Size of the vocabulary.\n",
        "      n: n-gram order.\n",
        "      alpha: Laplace smoothing parameter.\n",
        "\n",
        "    Returns:\n",
        "      A float representing the perplexity on the given dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    log_prob_sum = 0.0\n",
        "    num_tokens = 0  # Total number of tokens in the dataset\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        sentence = [\"<s>\"] * (n - 1) + sentence + [\"</s>\"]  # Add start & end tokens\n",
        "\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i : i + n])  # Extract n-gram\n",
        "            context = tuple(sentence[i : i + n - 1])  # Extract (n-1)-gram context\n",
        "\n",
        "            # Apply Laplace smoothing to compute probability\n",
        "            ngram_count = ngram_counts.get(ngram, 0) + alpha\n",
        "            context_count = context_counts.get(context, 0) + (alpha * vocab_size)\n",
        "\n",
        "            prob = ngram_count / context_count  # Compute probability\n",
        "            #prob = laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha)\n",
        "            log_prob_sum += math.log(prob)  # Sum log probabilities\n",
        "            num_tokens += 1\n",
        "\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / num_tokens) if num_tokens > 0 else float('inf')\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "_LN_xGAcGPnt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis**\n",
        "use different n and rerun the code and write down your analysis"
      ],
      "metadata": {
        "id": "__ExBYFpgj30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As\n",
        "𝑛\n",
        "n increases from 1 to 2, perplexity decreases because the model gains useful context for better predictions. However, for\n",
        "𝑛\n",
        ">\n",
        "2\n",
        "n>2, perplexity increases due to data sparsity, leading to poorly estimated probabilities"
      ],
      "metadata": {
        "id": "h_1hsbJAsE87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "ngram_counts, context_counts = build_ngram_counts(tokinized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Dr93S_qidm",
        "outputId": "b4d5ca5f-69b7-407b-d6d3-26748f200ffe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 161219\n",
            "Number of contexts: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test_sentences = tokinize(test_sentences, vocab)"
      ],
      "metadata": {
        "id": "M89OucIPmItb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity = calculate_perplexity(\n",
        "    tokenized_sentences=tokenized_test_sentences,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=1,\n",
        "    alpha=0.01\n",
        ")\n",
        "print(f\"Test Set Perplexity: {test_perplexity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt3yzH0djRXe",
        "outputId": "1bec47b9-6ac0-431f-9874-448d60dcb27a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Perplexity: 1267.2785636740116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2\n",
        "ngram_counts, context_counts = build_ngram_counts(tokinized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBm0spS4mNqO",
        "outputId": "c3381874-de77-4189-eb05-727ed3fc1926"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 2279086\n",
            "Number of contexts: 161219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity = calculate_perplexity(\n",
        "    tokenized_sentences=tokenized_test_sentences,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=2,\n",
        "    alpha=0.01\n",
        ")\n",
        "print(f\"Test Set Perplexity: {test_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w59exDS7qsrR",
        "outputId": "9fe86790-b1b3-467f-fb40-7fe3c916e282"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Perplexity: 660.5113838389991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "ngram_counts, context_counts = build_ngram_counts(tokinized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")"
      ],
      "metadata": {
        "id": "TeyqT1fTMwxs",
        "outputId": "6157aba0-6162-4d29-c80b-be75102b5caf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 6107483\n",
            "Number of contexts: 2270622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity = calculate_perplexity(\n",
        "    tokenized_sentences=tokenized_test_sentences,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=3,\n",
        "    alpha=0.01\n",
        ")\n",
        "print(f\"Test Set Perplexity: {test_perplexity}\")"
      ],
      "metadata": {
        "id": "H3dKcCPPM4gZ",
        "outputId": "4415d1db-1ad5-43af-af0e-016c95fee8fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Perplexity: 5883.583212908743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "ngram_counts, context_counts = build_ngram_counts(tokinized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")"
      ],
      "metadata": {
        "id": "EYnCezvjrwcl",
        "outputId": "f879fa2e-7d3f-4a1b-f929-fc132ea3fe12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 8807504\n",
            "Number of contexts: 6082216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity = calculate_perplexity(\n",
        "    tokenized_sentences=tokenized_test_sentences,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=4,\n",
        "    alpha=0.01\n",
        ")\n",
        "print(f\"Test Set Perplexity: {test_perplexity}\")"
      ],
      "metadata": {
        "id": "TJODiN-Ar0x2",
        "outputId": "32949c6a-bb20-4241-d216-4b11fa3698fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Set Perplexity: 40672.24893113284\n"
          ]
        }
      ]
    }
  ]
}