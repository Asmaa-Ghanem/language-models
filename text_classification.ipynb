{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7K0U7IuzWyZ"
   },
   "source": [
    "## Preprocessing for Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XifejkBXEdW9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b82eee82-126e-4e71-8014-05889e4089ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp7SBc5v5skq"
   },
   "source": [
    "- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5iYzfnnH5sHF"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zfhB4lP5zMZ"
   },
   "source": [
    "- Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "egJhiAZ0mpAn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d712931e-c476-452c-929c-174eac2df90a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/sst\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVBW_6NivGuu",
    "outputId": "ea9326b1-bcd0-42ce-d85e-c6463198eebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 8544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 2210\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDHqvx1Qv3ra"
   },
   "source": [
    "Converting dataset into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F8HUzDxvllmV"
   },
   "outputs": [],
   "source": [
    "train_text = np.array(dataset[\"train\"][\"sentence\"])\n",
    "train_labels = np.array(dataset[\"train\"][\"label\"])\n",
    "\n",
    "valid_text = np.array(dataset[\"validation\"][\"sentence\"])\n",
    "valid_labels = np.array(dataset[\"validation\"][\"label\"])\n",
    "\n",
    "test_text = np.array(dataset[\"test\"][\"sentence\"])\n",
    "test_labels = np.array(dataset[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wP1lI7zyrHv",
    "outputId": "39826af6-ba46-4280-9753-2386d62acb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6944400072097778\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvlSQVnHyMXr"
   },
   "source": [
    "Converting labels to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jjarj-mrl5Se"
   },
   "outputs": [],
   "source": [
    "bounds = np.array([0.2,0.4,0.6,0.8,1.0])\n",
    "train_labels = np.digitize(train_labels,bounds,True)\n",
    "valid_labels = np.digitize(valid_labels,bounds,True)\n",
    "test_labels = np.digitize(test_labels,bounds,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkl6jm89vQcE",
    "outputId": "4a96d284-65a0-4f8c-f1ff-1cfb296ee70c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 3 ... 3 0 1]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYxxWZYVzGfN"
   },
   "source": [
    "# Part 2.1: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aTvoWRJoy0uC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JkmssTTzN9J"
   },
   "source": [
    "# Part 2.2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMzsvkbnE5_b"
   },
   "source": [
    "## 2.2.1: Feauture Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sAXhWisX1ze"
   },
   "source": [
    "- Function to extract bi-grams from a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyBigUtbzSF-",
    "outputId": "f7ddc3d3-6fe4-4b9e-978c-6f67358d5e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-grams: [['i' 'love']\n",
      " ['love' 'this']\n",
      " ['this' 'movie']\n",
      " ['movie' 'very']\n",
      " ['very' 'much']\n",
      " ['much' 'i']\n",
      " ['i' 'love']]\n"
     ]
    }
   ],
   "source": [
    "def extract_bigrams(sentence):\n",
    "    words = np.array(sentence.lower().split())  # Convert sentence to lowercase and split\n",
    "    if len(words) < 2:\n",
    "        return np.array([])  # Return empty if less than two words\n",
    "    return np.column_stack((words[:-1], words[1:]))  # Create bi-grams # Remove duplicates and axis = 0 check uniqueness row-wise\n",
    "\n",
    "# Test the function\n",
    "sample_sentence = \"I love this movie very much I love\"\n",
    "print(\"Bi-grams:\", extract_bigrams(sample_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpDkzYc8ZZkm"
   },
   "source": [
    "- Construct a vocabulary of all unique bi-grams in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4nnvwIjY4DU",
    "outputId": "d6da334b-0b0a-4885-902d-022c76d025a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique bi-grams in vocabulary: 84518\n"
     ]
    }
   ],
   "source": [
    "# Extract all bi-grams from training sentences\n",
    "all_bigrams = np.concatenate([extract_bigrams(sentence) for sentence in train_text])\n",
    "\n",
    "# Get unique bi-grams\n",
    "unique_bigrams, indices = np.unique(all_bigrams, axis=0, return_index=True)\n",
    "\n",
    "# Create vocabulary using dictionary for easy indexing when creating vector\n",
    "bigram_vocab = {tuple(unique_bigrams[i]): i for i in range(len(unique_bigrams))}\n",
    "\n",
    "#print(bigram_vocab)\n",
    "print(f\"Total unique bi-grams in vocabulary: {len(bigram_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpN2VuUBbuJM"
   },
   "source": [
    "- Each sentence is then represented by a vector of length equal to the number of unique bi-grams, with a 1 if the bi-gram occurs in the sentence and a 0 otherwise (this is a sparse representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbRZrAYpZAz_",
    "outputId": "9ac45352-5531-44f4-fbe6-14fed69769ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentence: ! '\n",
      "Feature Vector: [1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_vector(sentence, vocab):\n",
    "    vector = np.zeros(len(vocab), dtype=np.int8)  # Initialize zero vector\n",
    "    bigrams = extract_bigrams(sentence)\n",
    "\n",
    "    if len(bigrams) == 0:\n",
    "        return vector  # Return empty vector if no bi-grams\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        bigram_tuple = tuple(bigram)\n",
    "        if bigram_tuple in vocab:\n",
    "            vector[vocab[bigram_tuple]] = 1  # Set 1 if bi-gram is in vocabulary\n",
    "    return vector\n",
    "\n",
    "# Test the function\n",
    "test_sentence = \"! '\"\n",
    "test_vector = sentence_to_vector(test_sentence, bigram_vocab)\n",
    "\n",
    "print(f\"Test Sentence: {test_sentence}\")\n",
    "print(f\"Feature Vector: {test_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycwebZ3oZFbX",
    "outputId": "6f194329-b1b7-4c04-b1d9-59e356792eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Feature Matrix Shape: (8544, 84518)\n",
      "Validation Feature Matrix Shape: (1101, 84518)\n",
      "Test Feature Matrix Shape: (2210, 84518)\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset\n",
    "X_train = np.array([sentence_to_vector(sentence, bigram_vocab) for sentence in train_text], dtype=np.float32)\n",
    "X_valid = np.array([sentence_to_vector(sentence, bigram_vocab) for sentence in valid_text], dtype=np.float32)\n",
    "X_test = np.array([sentence_to_vector(sentence, bigram_vocab) for sentence in test_text], dtype=np.float32)\n",
    "\n",
    "y_train = train_labels.astype(np.int8)\n",
    "y_valid = valid_labels.astype(np.int8)\n",
    "y_test = test_labels.astype(np.int8)\n",
    "\n",
    "print(f\"Train Feature Matrix Shape: {X_train.shape}\")\n",
    "print(f\"Validation Feature Matrix Shape: {X_valid.shape}\")\n",
    "print(f\"Test Feature Matrix Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij5-8uTGNkvS"
   },
   "source": [
    "## 2.2.2: Algorithm Implementation (From Scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCMAZ3Bzc11q"
   },
   "source": [
    "1. **Sigmoid Function:**\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "   where $z = W \\cdot X + b$.\n",
    "\n",
    "2. **Regularized Loss Function (Binary Cross-Entropy with L2 Regularization):**\n",
    "   $$\n",
    "   L_{reg} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} W_j^2\n",
    "   $$\n",
    "   where:\n",
    "   - $\\hat{y}_i = \\sigma(W \\cdot X_i + b)$\n",
    "   - $\\lambda$ is the regularization strength.\n",
    "   - $W_j^2$ penalizes large weights.\n",
    "\n",
    "3. **Gradient Updates with L2 Regularization:**\n",
    "   $$\n",
    "   W = W - \\alpha \\left( \\frac{\\partial L}{\\partial W} + \\lambda W \\right)\n",
    "   $$\n",
    "   $$\n",
    "   b = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "   $$\n",
    "   where:\n",
    "   - $\\frac{\\partial L}{\\partial W} = \\frac{1}{m} X^T (\\sigma(W \\cdot X + b) - y)$\n",
    "   - $\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum (\\sigma(W \\cdot X + b) - y) $\n",
    "   - $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcyidxbD7sXm",
    "outputId": "5a477ddf-9556-478e-cf57-6db0a3dda812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.5747337902874414\n",
      "Epoch 1, Loss: 1.5594941233395387\n",
      "Epoch 2, Loss: 1.5504207395072782\n",
      "Epoch 3, Loss: 1.5434409700497316\n",
      "Epoch 4, Loss: 1.537163121796506\n",
      "Epoch 5, Loss: 1.5312137831754506\n",
      "Epoch 6, Loss: 1.5254564813005107\n",
      "Epoch 7, Loss: 1.519808693507526\n",
      "Epoch 8, Loss: 1.5142564750473233\n",
      "Epoch 9, Loss: 1.5087877095267455\n",
      "Epoch 10, Loss: 1.5033964535638633\n",
      "Epoch 11, Loss: 1.498078820482594\n",
      "Epoch 12, Loss: 1.4928305811372198\n",
      "Epoch 13, Loss: 1.4876480776293393\n",
      "Epoch 14, Loss: 1.4825295953777677\n",
      "Epoch 15, Loss: 1.4774731909111143\n",
      "Epoch 16, Loss: 1.4724745204810348\n",
      "Epoch 17, Loss: 1.467530612005444\n",
      "Epoch 18, Loss: 1.4626416940423435\n",
      "Epoch 19, Loss: 1.457808223951343\n",
      "Epoch 20, Loss: 1.4530225426494405\n",
      "Epoch 21, Loss: 1.448287736698427\n",
      "Epoch 22, Loss: 1.4435994624890638\n",
      "Epoch 23, Loss: 1.4389582912417387\n",
      "Epoch 24, Loss: 1.4343615935608913\n",
      "Epoch 25, Loss: 1.4298076552621435\n",
      "Epoch 26, Loss: 1.425294875971945\n",
      "Epoch 27, Loss: 1.4208247411200838\n",
      "Epoch 28, Loss: 1.4163967933380202\n",
      "Epoch 29, Loss: 1.4120010889960586\n",
      "Epoch 30, Loss: 1.4076463144331166\n",
      "Epoch 31, Loss: 1.4033274400769995\n",
      "Epoch 32, Loss: 1.3990441229241841\n",
      "Epoch 33, Loss: 1.3947965913118048\n",
      "Epoch 34, Loss: 1.3905838168850553\n",
      "Epoch 35, Loss: 1.3864048848661146\n",
      "Epoch 36, Loss: 1.3822579663791013\n",
      "Epoch 37, Loss: 1.3781451187196296\n",
      "Epoch 38, Loss: 1.3740643985083933\n",
      "Epoch 39, Loss: 1.3700104338678527\n",
      "Epoch 40, Loss: 1.3659889393564915\n",
      "Epoch 41, Loss: 1.3619976289430809\n",
      "Epoch 42, Loss: 1.3580367311824126\n",
      "Epoch 43, Loss: 1.3541030455003964\n",
      "Epoch 44, Loss: 1.3501977151275129\n",
      "Epoch 45, Loss: 1.3463247401270082\n",
      "Epoch 46, Loss: 1.3424760047427278\n",
      "Epoch 47, Loss: 1.3386495661914124\n",
      "Epoch 48, Loss: 1.3348519396212664\n",
      "Epoch 49, Loss: 1.3310824389984546\n",
      "Epoch 50, Loss: 1.3273412927717305\n",
      "Epoch 51, Loss: 1.3236190140068753\n",
      "Epoch 52, Loss: 1.3199292038107633\n",
      "Epoch 53, Loss: 1.3162556323152357\n",
      "Epoch 54, Loss: 1.3126113291068024\n",
      "Epoch 55, Loss: 1.3089920654971055\n",
      "Epoch 56, Loss: 1.3053923544432786\n",
      "Epoch 57, Loss: 1.301816996519039\n",
      "Epoch 58, Loss: 1.2982706775647423\n",
      "Epoch 59, Loss: 1.2947444831342734\n",
      "Epoch 60, Loss: 1.2912298405884803\n",
      "Epoch 61, Loss: 1.2877472086658193\n",
      "Epoch 62, Loss: 1.2842864147420232\n",
      "Epoch 63, Loss: 1.2808444876818175\n",
      "Epoch 64, Loss: 1.2774242841527703\n",
      "Epoch 65, Loss: 1.2740255754806575\n",
      "Epoch 66, Loss: 1.2706475620010818\n",
      "Epoch 67, Loss: 1.2672943576779705\n",
      "Epoch 68, Loss: 1.2639553336748914\n",
      "Epoch 69, Loss: 1.260640090186051\n",
      "Epoch 70, Loss: 1.2573439408487148\n",
      "Epoch 71, Loss: 1.254066428977452\n",
      "Epoch 72, Loss: 1.250809497013967\n",
      "Epoch 73, Loss: 1.2475718882847369\n",
      "Epoch 74, Loss: 1.2443540590130882\n",
      "Epoch 75, Loss: 1.2411540667238308\n",
      "Epoch 76, Loss: 1.2379739682727986\n",
      "Epoch 77, Loss: 1.2348119352328195\n",
      "Epoch 78, Loss: 1.2316689959715368\n",
      "Epoch 79, Loss: 1.228543664489346\n",
      "Epoch 80, Loss: 1.225435826376583\n",
      "Epoch 81, Loss: 1.2223461675867158\n",
      "Epoch 82, Loss: 1.2192735451125027\n",
      "Epoch 83, Loss: 1.216220015832101\n",
      "Epoch 84, Loss: 1.2131823799047579\n",
      "Epoch 85, Loss: 1.210162351363607\n",
      "Epoch 86, Loss: 1.2071601585993608\n",
      "Epoch 87, Loss: 1.2041739733076275\n",
      "Epoch 88, Loss: 1.2012050523516837\n",
      "Epoch 89, Loss: 1.19825442437822\n",
      "Epoch 90, Loss: 1.1953161454423984\n",
      "Epoch 91, Loss: 1.1924017605397585\n",
      "Epoch 92, Loss: 1.1894936663827647\n",
      "Epoch 93, Loss: 1.1866048936629563\n",
      "Epoch 94, Loss: 1.1837353280108518\n",
      "Epoch 95, Loss: 1.1808773118547733\n",
      "Epoch 96, Loss: 1.1780365593201214\n",
      "Epoch 97, Loss: 1.1752131851826715\n",
      "Epoch 98, Loss: 1.1724022739746152\n",
      "Epoch 99, Loss: 1.169605883074164\n",
      "Logistic Regression Accuracy: 34.705882352941174\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, lr=0.01, epochs=100, batch_size=32, reg_lambda=0.01):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        m = y.shape[0]\n",
    "        return -np.sum(y * np.log(y_pred + 1e-8)) / m + (self.reg_lambda / (2 * m)) * np.sum(self.weights ** 2)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        self.weights = np.zeros((n_features, n_classes), dtype=np.float32)\n",
    "        self.bias = np.zeros(n_classes, dtype=np.float32)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        y_one_hot = np.eye(n_classes, dtype=np.float32)[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X, y_one_hot = X[indices], y_one_hot[indices]\n",
    "\n",
    "            del indices  # Free memory\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                y_batch = y_one_hot[i:i+self.batch_size]\n",
    "\n",
    "                y_pred = self.softmax(np.dot(X_batch, self.weights) + self.bias)\n",
    "\n",
    "                dw = (1 / len(y_batch)) * np.dot(X_batch.T, (y_pred - y_batch)) + (self.reg_lambda / len(y_batch)) * self.weights\n",
    "                db = np.mean(y_pred - y_batch, axis=0)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            loss = self.loss(y_one_hot, self.softmax(np.dot(X, self.weights) + self.bias))\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.softmax(np.dot(X, self.weights) + self.bias), axis=1)\n",
    "\n",
    "model = LogisticRegressionSGD(lr=0.01, epochs=100, batch_size=32, reg_lambda=0.01)\n",
    "model.train(X_train, train_labels)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == test_labels)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuZLqy2-8DNS"
   },
   "source": [
    "- Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGbYmDCGWUTM",
    "outputId": "cfb56d53-0845-47c1-9cf6-b2c3f14c517c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.574819327209996\n",
      "Epoch 1, Loss: 1.5596076986902438\n",
      "Epoch 2, Loss: 1.5504468274529508\n",
      "Epoch 3, Loss: 1.543409004277281\n",
      "Epoch 4, Loss: 1.5371144132109618\n",
      "Epoch 5, Loss: 1.5311484856719382\n",
      "Epoch 6, Loss: 1.5253421623217926\n",
      "Epoch 7, Loss: 1.5196614747427488\n",
      "Epoch 8, Loss: 1.5140680398805981\n",
      "Epoch 9, Loss: 1.5085544687127241\n",
      "Epoch 10, Loss: 1.5031149267648043\n",
      "Epoch 11, Loss: 1.4977461418018359\n",
      "Epoch 12, Loss: 1.492445072194052\n",
      "Epoch 13, Loss: 1.4872084564707253\n",
      "Epoch 14, Loss: 1.4820280785947926\n",
      "Epoch 15, Loss: 1.4769105395016153\n",
      "Epoch 16, Loss: 1.4718455534065595\n",
      "Epoch 17, Loss: 1.4668329759441623\n",
      "Epoch 18, Loss: 1.4618741271298812\n",
      "Epoch 19, Loss: 1.4569658991158678\n",
      "Epoch 20, Loss: 1.4521022377054342\n",
      "Epoch 21, Loss: 1.4472878900555413\n",
      "Epoch 22, Loss: 1.4425129565839352\n",
      "Epoch 23, Loss: 1.4377849592098249\n",
      "Epoch 24, Loss: 1.4330988302966405\n",
      "Epoch 25, Loss: 1.428452049064614\n",
      "Epoch 26, Loss: 1.4238477860078607\n",
      "Epoch 27, Loss: 1.419278154651938\n",
      "Epoch 28, Loss: 1.4147495119745253\n",
      "Epoch 29, Loss: 1.410255788605702\n",
      "Epoch 30, Loss: 1.4057961239475993\n",
      "Epoch 31, Loss: 1.4013653689235002\n",
      "Epoch 32, Loss: 1.3969752415665861\n",
      "Epoch 33, Loss: 1.3926145627969102\n",
      "Epoch 34, Loss: 1.3882870064204218\n",
      "Epoch 35, Loss: 1.3839926304790968\n",
      "Epoch 36, Loss: 1.37972597705291\n",
      "Epoch 37, Loss: 1.3754909088493286\n",
      "Epoch 38, Loss: 1.3712855732689302\n",
      "Epoch 39, Loss: 1.3671082073494736\n",
      "Epoch 40, Loss: 1.3629607349592314\n",
      "Epoch 41, Loss: 1.3588407953629054\n",
      "Epoch 42, Loss: 1.3547489131811294\n",
      "Epoch 43, Loss: 1.350680326401127\n",
      "Epoch 44, Loss: 1.34664109748987\n",
      "Epoch 45, Loss: 1.3426244783455903\n",
      "Epoch 46, Loss: 1.3386351530115759\n",
      "Epoch 47, Loss: 1.3346729112984368\n",
      "Epoch 48, Loss: 1.3307342859864066\n",
      "Epoch 49, Loss: 1.3268175986996584\n",
      "Epoch 50, Loss: 1.3229276289089063\n",
      "Epoch 51, Loss: 1.3190576588274556\n",
      "Epoch 52, Loss: 1.3152120673212777\n",
      "Epoch 53, Loss: 1.3113915870796318\n",
      "Epoch 54, Loss: 1.307591364516664\n",
      "Epoch 55, Loss: 1.3038150559908497\n",
      "Epoch 56, Loss: 1.3000591864900084\n",
      "Epoch 57, Loss: 1.2963261770900716\n",
      "Epoch 58, Loss: 1.292615231866024\n",
      "Epoch 59, Loss: 1.2889233095857422\n",
      "Epoch 60, Loss: 1.2852533042312282\n",
      "Epoch 61, Loss: 1.2816036853150985\n",
      "Epoch 62, Loss: 1.2779769013925326\n",
      "Epoch 63, Loss: 1.2743656506678194\n",
      "Epoch 64, Loss: 1.270778654621084\n",
      "Epoch 65, Loss: 1.2672095302429593\n",
      "Epoch 66, Loss: 1.263658021806917\n",
      "Epoch 67, Loss: 1.2601273457767366\n",
      "Epoch 68, Loss: 1.2566193866260935\n",
      "Epoch 69, Loss: 1.2531239018646463\n",
      "Epoch 70, Loss: 1.2496483553103293\n",
      "Epoch 71, Loss: 1.2461924273834637\n",
      "Epoch 72, Loss: 1.2427550692369373\n",
      "Epoch 73, Loss: 1.239336892240419\n",
      "Epoch 74, Loss: 1.2359378247483448\n",
      "Epoch 75, Loss: 1.2325577373487338\n",
      "Epoch 76, Loss: 1.2291860912127721\n",
      "Epoch 77, Loss: 1.2258366656486182\n",
      "Epoch 78, Loss: 1.2225064502345322\n",
      "Epoch 79, Loss: 1.219190422278107\n",
      "Epoch 80, Loss: 1.2158943334880392\n",
      "Epoch 81, Loss: 1.212613523313392\n",
      "Epoch 82, Loss: 1.2093503274030508\n",
      "Epoch 83, Loss: 1.206101244038201\n",
      "Epoch 84, Loss: 1.2028688180577778\n",
      "Epoch 85, Loss: 1.1996522748717264\n",
      "Epoch 86, Loss: 1.1964532666567587\n",
      "Epoch 87, Loss: 1.193270998761016\n",
      "Epoch 88, Loss: 1.1901047263740936\n",
      "Epoch 89, Loss: 1.1869511076716588\n",
      "Epoch 90, Loss: 1.1838136370806547\n",
      "Epoch 91, Loss: 1.18069244702678\n",
      "Epoch 92, Loss: 1.1775852208119932\n",
      "Epoch 93, Loss: 1.1744946315692744\n",
      "Epoch 94, Loss: 1.1714194468676011\n",
      "Epoch 95, Loss: 1.1683572072915955\n",
      "Epoch 96, Loss: 1.165312934598854\n",
      "Epoch 97, Loss: 1.1622799196304956\n",
      "Epoch 98, Loss: 1.1592620984417445\n",
      "Epoch 99, Loss: 1.15626321813573\n",
      "Logistic Regression Accuracy: 34.705882352941174\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, lr=0.01, epochs=100, batch_size=32):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        m = y.shape[0]\n",
    "        return -np.sum(y * np.log(y_pred + 1e-8)) / m\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        self.weights = np.zeros((n_features, n_classes), dtype=np.float32)\n",
    "        self.bias = np.zeros(n_classes, dtype=np.float32)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        y_one_hot = np.eye(n_classes)[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X, y_one_hot = X[indices], y_one_hot[indices]\n",
    "\n",
    "            del indices\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                y_batch = y_one_hot[i:i+self.batch_size]\n",
    "\n",
    "                y_pred = self.softmax(np.dot(X_batch, self.weights) + self.bias)\n",
    "\n",
    "                dw = (1 / len(y_batch)) * np.dot(X_batch.T, (y_pred - y_batch))\n",
    "                db = np.mean(y_pred - y_batch, axis=0)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            loss = self.loss(y_one_hot, self.softmax(np.dot(X, self.weights) + self.bias))\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.softmax(np.dot(X, self.weights) + self.bias), axis=1)\n",
    "\n",
    "# Train model with Softmax and Cross-Entropy Loss\n",
    "model = LogisticRegressionSGD(lr=0.01, epochs=100, batch_size=32)\n",
    "model.train(X_train, train_labels)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == test_labels)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9mTDy3mNt-Q"
   },
   "source": [
    "- ## Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MU8cm_sO9rI"
   },
   "source": [
    "Since **X_train** is a high-dimensional feature matrix (one-hot encoded bigrams).\n",
    "Most values in this matrix are zero (because each sentence contains only a few bigrams out of a huge vocabulary).\n",
    "- Dense storage wastes memory by keeping all the zeros.\n",
    "\n",
    "- Sparse matrices solve this by only storing nonzero values and their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-L5PoSoIQf0B",
    "outputId": "928fe50e-43fb-4c9c-de49-6bdc432e4eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy (Batch Gradient Descent): 36.10859728506787\n",
      "Logistic Regression Accuracy (Stochastic Gradient Descent): 35.97285067873303\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "X_train = csr_matrix(X_train)\n",
    "X_test = csr_matrix(X_test)\n",
    "\n",
    "bgd = LogisticRegression(max_iter=100, solver='lbfgs')\n",
    "bgd.fit(X_train, train_labels)\n",
    "bgd_accuracy = bgd.score(X_test, test_labels)\n",
    "\n",
    "sgd = SGDClassifier(loss=\"log_loss\", max_iter=100, learning_rate=\"optimal\")\n",
    "sgd.fit(X_train, train_labels)\n",
    "sgd_accuracy = sgd.score(X_test, test_labels)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy (Batch Gradient Descent): {bgd_accuracy*100}\")\n",
    "print(f\"Logistic Regression Accuracy (Stochastic Gradient Descent): {sgd_accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXoH04UoPP1l"
   },
   "source": [
    "# Part 2.3: Confusion Matrix & Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6AEC2U9U0vt"
   },
   "source": [
    "- Implementation (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OcaNM_VHSZHd"
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[true, pred] += 1\n",
    "    return matrix\n",
    "\n",
    "def compute_metrics(conf_matrix):\n",
    "    precision = np.diag(conf_matrix) / (np.sum(conf_matrix, axis=0) + 1e-8)\n",
    "    recall = np.diag(conf_matrix) / (np.sum(conf_matrix, axis=1) + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1)\n",
    "\n",
    "    return precision, recall, f1, macro_precision, macro_recall, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byvTypY9U_sY",
    "outputId": "40790e2a-a197-4ba9-87dd-15532c7dd0e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From scratch Confusion Matrix:\n",
      "[[  3 172   7  92   5]\n",
      " [  2 372  27 226   6]\n",
      " [  2 194  12 173   8]\n",
      " [  1 139  21 340   9]\n",
      " [  0  92   9 258  40]]\n",
      "Precision per class: [0.375      0.38390093 0.15789474 0.31221304 0.58823529]\n",
      "Recall per class: [0.01075269 0.58767773 0.03084833 0.66666667 0.10025063]\n",
      "F1-score per class: [0.02090592 0.46441947 0.0516129  0.42526579 0.17130621]\n",
      "Macro Precision: 0.3634487997310459\n",
      "Macro Recall: 0.279239207109281\n",
      "Macro F1-score: 0.22670205765689583\n"
     ]
    }
   ],
   "source": [
    "# From scratch confusion matrix and metrics\n",
    "num_classes = len(np.unique(test_labels))\n",
    "conf_matrix = compute_confusion_matrix(test_labels, y_pred, num_classes)\n",
    "precision, recall, f1, macro_precision, macro_recall, macro_f1 = compute_metrics(conf_matrix)\n",
    "\n",
    "print(\"From scratch Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision per class: {precision}\")\n",
    "print(f\"Recall per class: {recall}\")\n",
    "print(f\"F1-score per class: {f1}\")\n",
    "print(f\"Macro Precision: {macro_precision}\")\n",
    "print(f\"Macro Recall: {macro_recall}\")\n",
    "print(f\"Macro F1-score: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXyAMWdoUpcV"
   },
   "source": [
    "- Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CGwD2iuUp_c",
    "outputId": "d0b82324-9725-4084-8631-449d6daaf7a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Confusion Matrix:\n",
      "[[  3 172   7  92   5]\n",
      " [  2 372  27 226   6]\n",
      " [  2 194  12 173   8]\n",
      " [  1 139  21 340   9]\n",
      " [  0  92   9 258  40]]\n",
      "Precision per class: [0.375      0.38390093 0.15789474 0.31221304 0.58823529]\n",
      "Recall per class: [0.01075269 0.58767773 0.03084833 0.66666667 0.10025063]\n",
      "F1-score per class: [0.02090592 0.46441948 0.0516129  0.42526579 0.17130621]\n",
      "Macro Precision: 0.36344879984761774\n",
      "Macro Recall: 0.27923920711449046\n",
      "Macro F1-score: 0.22670206063914833\n"
     ]
    }
   ],
   "source": [
    "# Compare with sklearn's metrics\n",
    "sklearn_conf_matrix = confusion_matrix(test_labels, y_pred)\n",
    "sklearn_precision = precision_score(test_labels, y_pred, average=None)\n",
    "sklearn_recall = recall_score(test_labels, y_pred, average=None)\n",
    "sklearn_f1 = f1_score(test_labels, y_pred, average=None)\n",
    "sklearn_macro_precision = precision_score(test_labels, y_pred, average=\"macro\")\n",
    "sklearn_macro_recall = recall_score(test_labels, y_pred, average=\"macro\")\n",
    "sklearn_macro_f1 = f1_score(test_labels, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"Scikit-learn Confusion Matrix:\")\n",
    "print(sklearn_conf_matrix)\n",
    "print(f\"Precision per class: {sklearn_precision}\")\n",
    "print(f\"Recall per class: {sklearn_recall}\")\n",
    "print(f\"F1-score per class: {sklearn_f1}\")\n",
    "print(f\"Macro Precision: {sklearn_macro_precision}\")\n",
    "print(f\"Macro Recall: {sklearn_macro_recall}\")\n",
    "print(f\"Macro F1-score: {sklearn_macro_f1}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
